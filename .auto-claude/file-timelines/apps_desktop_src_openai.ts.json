{
  "file_path": "apps/desktop/src/openai.ts",
  "main_branch_history": [],
  "task_views": {
    "007-so-on-the-engine-right-here-where-i-chat-so-here-l": {
      "task_id": "007-so-on-the-engine-right-here-where-i-chat-so-here-l",
      "branch_point": {
        "commit_hash": "223b6906c6e567162b9c4367984c6ffbc6ed28e6",
        "content": "export type OpenAiMessage = {\n  role: 'system' | 'user' | 'assistant'\n  content: string\n}\n\nexport function openAiApiUrl(path: string) {\n  const base =\n    typeof window !== 'undefined' && window.location?.protocol?.startsWith('http')\n      ? '/openai'\n      : 'https://api.openai.com'\n  const suffix = path.startsWith('/') ? path : `/${path}`\n  return `${base}${suffix}`\n}\n\nfunction extractResponseText(json: any): string {\n  if (typeof json?.output_text === 'string' && json.output_text) return json.output_text\n  const parts: string[] = []\n  const out = json?.output\n  if (Array.isArray(out)) {\n    for (const item of out) {\n      if (item?.type !== 'message') continue\n      if (item?.role !== 'assistant') continue\n      const content = item?.content\n      if (Array.isArray(content)) {\n        for (const c of content) {\n          if (typeof c === 'string') parts.push(c)\n          else if (c?.type === 'output_text' && typeof c?.text === 'string') parts.push(c.text)\n          else if (c?.type === 'text' && typeof c?.text === 'string') parts.push(c.text)\n        }\n      } else if (typeof content === 'string') {\n        parts.push(content)\n      }\n    }\n  }\n  return parts.join('') || ''\n}\n\nasync function callChatCompletionsFallback(opts: {\n  apiKey: string\n  model: string\n  messages: OpenAiMessage[]\n  temperature: number\n  maxOutputTokens: number\n  responseFormat?: { type: 'json_object' } | null\n}) {\n  const model = opts.model.trim()\n  const useMaxCompletionTokens = /^gpt-5/i.test(model)\n  const body: Record<string, unknown> = {\n    model,\n    messages: opts.messages,\n    ...(useMaxCompletionTokens ? { max_completion_tokens: opts.maxOutputTokens } : { max_tokens: opts.maxOutputTokens }),\n    ...(opts.responseFormat ? { response_format: opts.responseFormat } : {}),\n  }\n  const supportsTemperature = !/^gpt-5/i.test(model) && !/^o[1-9]/i.test(model)\n  if (supportsTemperature) body.temperature = opts.temperature\n\n  let res = await fetch(openAiApiUrl('/v1/chat/completions'), {\n    method: 'POST',\n    headers: {\n      'Content-Type': 'application/json',\n      Authorization: `Bearer ${opts.apiKey}`,\n    },\n    body: JSON.stringify(body),\n  })\n\n  if (!res.ok) {\n    const text = await res.text().catch(() => '')\n    const isTempError = text.includes('Unsupported parameter') && text.includes('temperature')\n    const isMaxTokenError = text.includes('max_tokens') || text.includes('max_completion_tokens')\n    const isResponseFormatError = text.includes('response_format')\n    const retryBody = { ...body }\n    let shouldRetry = false\n    if (isTempError && 'temperature' in retryBody) {\n      delete retryBody.temperature\n      shouldRetry = true\n    }\n    if (isResponseFormatError && 'response_format' in retryBody) {\n      delete retryBody.response_format\n      shouldRetry = true\n    }\n    if (isMaxTokenError) {\n      if ('max_tokens' in retryBody && !('max_completion_tokens' in retryBody)) {\n        retryBody.max_completion_tokens = retryBody.max_tokens\n        delete retryBody.max_tokens\n        shouldRetry = true\n      } else if ('max_completion_tokens' in retryBody && !('max_tokens' in retryBody)) {\n        retryBody.max_tokens = retryBody.max_completion_tokens\n        delete retryBody.max_completion_tokens\n        shouldRetry = true\n      }\n    }\n    if (shouldRetry) {\n      res = await fetch(openAiApiUrl('/v1/chat/completions'), {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json',\n          Authorization: `Bearer ${opts.apiKey}`,\n        },\n        body: JSON.stringify(retryBody),\n      })\n      if (!res.ok) {\n        const retryText = await res.text().catch(() => '')\n        throw new Error(`OpenAI HTTP ${res.status}: ${retryText.slice(0, 240)}`)\n      }\n    } else {\n      throw new Error(`OpenAI HTTP ${res.status}: ${text.slice(0, 240)}`)\n    }\n  }\n  const json = (await res.json()) as any\n  return (json?.choices?.[0]?.message?.content as string | undefined) ?? ''\n}\n\nexport async function callOpenAiText(opts: {\n  apiKey: string\n  model: string\n  messages: OpenAiMessage[]\n  temperature?: number\n  maxOutputTokens?: number\n  responseFormat?: { type: 'json_object' } | null\n}) {\n  const model = opts.model.trim()\n  const temperature = opts.temperature ?? 0.2\n  const maxOutputTokens = opts.maxOutputTokens ?? 800\n  const supportsTemperature = !/^gpt-5/i.test(model) && !/^o[1-9]/i.test(model)\n  const supportsResponseFormat = !/^gpt-5/i.test(model) && !/^o[1-9]/i.test(model)\n\n  if (opts.responseFormat && supportsResponseFormat) {\n    return await callChatCompletionsFallback({\n      apiKey: opts.apiKey,\n      model,\n      messages: opts.messages,\n      temperature,\n      maxOutputTokens,\n      responseFormat: opts.responseFormat,\n    })\n  }\n\n  const res = await fetch(openAiApiUrl('/v1/responses'), {\n    method: 'POST',\n    headers: {\n      'Content-Type': 'application/json',\n      Authorization: `Bearer ${opts.apiKey}`,\n    },\n    body: JSON.stringify({\n      model,\n      input: opts.messages,\n      ...(supportsTemperature ? { temperature } : {}),\n      max_output_tokens: maxOutputTokens,\n    }),\n  })\n\n  if (res.status === 404) {\n    return await callChatCompletionsFallback({\n      apiKey: opts.apiKey,\n      model: opts.model,\n      messages: opts.messages,\n      temperature,\n      maxOutputTokens,\n      responseFormat: opts.responseFormat ?? null,\n    })\n  }\n\n  if (!res.ok) {\n    const text = await res.text().catch(() => '')\n    throw new Error(`OpenAI HTTP ${res.status}: ${text.slice(0, 240)}`)\n  }\n  const json = (await res.json()) as any\n  return extractResponseText(json)\n}\n\nexport async function callOpenAiEmbedding(opts: { apiKey: string; model: string; input: string }) {\n  const res = await fetch(openAiApiUrl('/v1/embeddings'), {\n    method: 'POST',\n    headers: {\n      'Content-Type': 'application/json',\n      Authorization: `Bearer ${opts.apiKey}`,\n    },\n    body: JSON.stringify({\n      model: opts.model,\n      input: opts.input,\n    }),\n  })\n\n  if (!res.ok) {\n    const text = await res.text().catch(() => '')\n    throw new Error(`OpenAI HTTP ${res.status}: ${text.slice(0, 240)}`)\n  }\n\n  const json = (await res.json()) as any\n  const embedding = json?.data?.[0]?.embedding\n  if (!Array.isArray(embedding)) {\n    throw new Error('OpenAI embedding response missing embedding data.')\n  }\n  return embedding as number[]\n}\n",
        "timestamp": "2026-01-13T12:35:10.658231"
      },
      "worktree_state": {
        "content": "export type OpenAiMessage = {\n  role: 'system' | 'user' | 'assistant'\n  content: string\n}\n\nexport type MultiModalContent =\n  | { type: 'text'; text: string }\n  | { type: 'image_url'; image_url: { url: string; detail?: 'low' | 'high' | 'auto' } }\n\nexport type MultiModalMessage = {\n  role: 'system' | 'user' | 'assistant'\n  content: string | MultiModalContent[]\n}\n\nexport function openAiApiUrl(path: string) {\n  const base =\n    typeof window !== 'undefined' && window.location?.protocol?.startsWith('http')\n      ? '/openai'\n      : 'https://api.openai.com'\n  const suffix = path.startsWith('/') ? path : `/${path}`\n  return `${base}${suffix}`\n}\n\nfunction extractResponseText(json: any): string {\n  if (typeof json?.output_text === 'string' && json.output_text) return json.output_text\n  const parts: string[] = []\n  const out = json?.output\n  if (Array.isArray(out)) {\n    for (const item of out) {\n      if (item?.type !== 'message') continue\n      if (item?.role !== 'assistant') continue\n      const content = item?.content\n      if (Array.isArray(content)) {\n        for (const c of content) {\n          if (typeof c === 'string') parts.push(c)\n          else if (c?.type === 'output_text' && typeof c?.text === 'string') parts.push(c.text)\n          else if (c?.type === 'text' && typeof c?.text === 'string') parts.push(c.text)\n        }\n      } else if (typeof content === 'string') {\n        parts.push(content)\n      }\n    }\n  }\n  return parts.join('') || ''\n}\n\nasync function callChatCompletionsFallback(opts: {\n  apiKey: string\n  model: string\n  messages: OpenAiMessage[]\n  temperature: number\n  maxOutputTokens: number\n  responseFormat?: { type: 'json_object' } | null\n}) {\n  const model = opts.model.trim()\n  const useMaxCompletionTokens = /^gpt-5/i.test(model)\n  const body: Record<string, unknown> = {\n    model,\n    messages: opts.messages,\n    ...(useMaxCompletionTokens ? { max_completion_tokens: opts.maxOutputTokens } : { max_tokens: opts.maxOutputTokens }),\n    ...(opts.responseFormat ? { response_format: opts.responseFormat } : {}),\n  }\n  const supportsTemperature = !/^gpt-5/i.test(model) && !/^o[1-9]/i.test(model)\n  if (supportsTemperature) body.temperature = opts.temperature\n\n  let res = await fetch(openAiApiUrl('/v1/chat/completions'), {\n    method: 'POST',\n    headers: {\n      'Content-Type': 'application/json',\n      Authorization: `Bearer ${opts.apiKey}`,\n    },\n    body: JSON.stringify(body),\n  })\n\n  if (!res.ok) {\n    const text = await res.text().catch(() => '')\n    const isTempError = text.includes('Unsupported parameter') && text.includes('temperature')\n    const isMaxTokenError = text.includes('max_tokens') || text.includes('max_completion_tokens')\n    const isResponseFormatError = text.includes('response_format')\n    const retryBody = { ...body }\n    let shouldRetry = false\n    if (isTempError && 'temperature' in retryBody) {\n      delete retryBody.temperature\n      shouldRetry = true\n    }\n    if (isResponseFormatError && 'response_format' in retryBody) {\n      delete retryBody.response_format\n      shouldRetry = true\n    }\n    if (isMaxTokenError) {\n      if ('max_tokens' in retryBody && !('max_completion_tokens' in retryBody)) {\n        retryBody.max_completion_tokens = retryBody.max_tokens\n        delete retryBody.max_tokens\n        shouldRetry = true\n      } else if ('max_completion_tokens' in retryBody && !('max_tokens' in retryBody)) {\n        retryBody.max_tokens = retryBody.max_completion_tokens\n        delete retryBody.max_completion_tokens\n        shouldRetry = true\n      }\n    }\n    if (shouldRetry) {\n      res = await fetch(openAiApiUrl('/v1/chat/completions'), {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json',\n          Authorization: `Bearer ${opts.apiKey}`,\n        },\n        body: JSON.stringify(retryBody),\n      })\n      if (!res.ok) {\n        const retryText = await res.text().catch(() => '')\n        throw new Error(`OpenAI HTTP ${res.status}: ${retryText.slice(0, 240)}`)\n      }\n    } else {\n      throw new Error(`OpenAI HTTP ${res.status}: ${text.slice(0, 240)}`)\n    }\n  }\n  const json = (await res.json()) as any\n  return (json?.choices?.[0]?.message?.content as string | undefined) ?? ''\n}\n\nexport async function callOpenAiText(opts: {\n  apiKey: string\n  model: string\n  messages: OpenAiMessage[]\n  temperature?: number\n  maxOutputTokens?: number\n  responseFormat?: { type: 'json_object' } | null\n}) {\n  const model = opts.model.trim()\n  const temperature = opts.temperature ?? 0.2\n  const maxOutputTokens = opts.maxOutputTokens ?? 800\n  const supportsTemperature = !/^gpt-5/i.test(model) && !/^o[1-9]/i.test(model)\n  const supportsResponseFormat = !/^gpt-5/i.test(model) && !/^o[1-9]/i.test(model)\n\n  if (opts.responseFormat && supportsResponseFormat) {\n    return await callChatCompletionsFallback({\n      apiKey: opts.apiKey,\n      model,\n      messages: opts.messages,\n      temperature,\n      maxOutputTokens,\n      responseFormat: opts.responseFormat,\n    })\n  }\n\n  const res = await fetch(openAiApiUrl('/v1/responses'), {\n    method: 'POST',\n    headers: {\n      'Content-Type': 'application/json',\n      Authorization: `Bearer ${opts.apiKey}`,\n    },\n    body: JSON.stringify({\n      model,\n      input: opts.messages,\n      ...(supportsTemperature ? { temperature } : {}),\n      max_output_tokens: maxOutputTokens,\n    }),\n  })\n\n  if (res.status === 404) {\n    return await callChatCompletionsFallback({\n      apiKey: opts.apiKey,\n      model: opts.model,\n      messages: opts.messages,\n      temperature,\n      maxOutputTokens,\n      responseFormat: opts.responseFormat ?? null,\n    })\n  }\n\n  if (!res.ok) {\n    const text = await res.text().catch(() => '')\n    throw new Error(`OpenAI HTTP ${res.status}: ${text.slice(0, 240)}`)\n  }\n  const json = (await res.json()) as any\n  return extractResponseText(json)\n}\n\nexport async function callOpenAiEmbedding(opts: { apiKey: string; model: string; input: string }) {\n  const res = await fetch(openAiApiUrl('/v1/embeddings'), {\n    method: 'POST',\n    headers: {\n      'Content-Type': 'application/json',\n      Authorization: `Bearer ${opts.apiKey}`,\n    },\n    body: JSON.stringify({\n      model: opts.model,\n      input: opts.input,\n    }),\n  })\n\n  if (!res.ok) {\n    const text = await res.text().catch(() => '')\n    throw new Error(`OpenAI HTTP ${res.status}: ${text.slice(0, 240)}`)\n  }\n\n  const json = (await res.json()) as any\n  const embedding = json?.data?.[0]?.embedding\n  if (!Array.isArray(embedding)) {\n    throw new Error('OpenAI embedding response missing embedding data.')\n  }\n  return embedding as number[]\n}\n\nexport async function callOpenAiVision(opts: {\n  apiKey: string\n  model: string\n  messages: MultiModalMessage[]\n  temperature?: number\n  maxOutputTokens?: number\n}) {\n  const model = opts.model.trim()\n  const temperature = opts.temperature ?? 0.2\n  const maxOutputTokens = opts.maxOutputTokens ?? 800\n  const supportsTemperature = !/^gpt-5/i.test(model) && !/^o[1-9]/i.test(model)\n  const useMaxCompletionTokens = /^gpt-5/i.test(model)\n\n  const body: Record<string, unknown> = {\n    model,\n    messages: opts.messages,\n    ...(useMaxCompletionTokens\n      ? { max_completion_tokens: maxOutputTokens }\n      : { max_tokens: maxOutputTokens }),\n  }\n  if (supportsTemperature) body.temperature = temperature\n\n  let res = await fetch(openAiApiUrl('/v1/chat/completions'), {\n    method: 'POST',\n    headers: {\n      'Content-Type': 'application/json',\n      Authorization: `Bearer ${opts.apiKey}`,\n    },\n    body: JSON.stringify(body),\n  })\n\n  if (!res.ok) {\n    const text = await res.text().catch(() => '')\n    const isTempError = text.includes('Unsupported parameter') && text.includes('temperature')\n    const isMaxTokenError = text.includes('max_tokens') || text.includes('max_completion_tokens')\n    const retryBody = { ...body }\n    let shouldRetry = false\n    if (isTempError && 'temperature' in retryBody) {\n      delete retryBody.temperature\n      shouldRetry = true\n    }\n    if (isMaxTokenError) {\n      if ('max_tokens' in retryBody && !('max_completion_tokens' in retryBody)) {\n        retryBody.max_completion_tokens = retryBody.max_tokens\n        delete retryBody.max_tokens\n        shouldRetry = true\n      } else if ('max_completion_tokens' in retryBody && !('max_tokens' in retryBody)) {\n        retryBody.max_tokens = retryBody.max_completion_tokens\n        delete retryBody.max_completion_tokens\n        shouldRetry = true\n      }\n    }\n    if (shouldRetry) {\n      res = await fetch(openAiApiUrl('/v1/chat/completions'), {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json',\n          Authorization: `Bearer ${opts.apiKey}`,\n        },\n        body: JSON.stringify(retryBody),\n      })\n      if (!res.ok) {\n        const retryText = await res.text().catch(() => '')\n        throw new Error(`OpenAI HTTP ${res.status}: ${retryText.slice(0, 240)}`)\n      }\n    } else {\n      throw new Error(`OpenAI HTTP ${res.status}: ${text.slice(0, 240)}`)\n    }\n  }\n\n  const json = (await res.json()) as any\n  return (json?.choices?.[0]?.message?.content as string | undefined) ?? ''\n}\n\nexport async function callOpenAiWhisper(opts: {\n  apiKey: string\n  model: string\n  audioBlob: Blob\n  language?: string\n  prompt?: string\n}) {\n  const formData = new FormData()\n  formData.append('file', opts.audioBlob, 'audio.webm')\n  formData.append('model', opts.model)\n  if (opts.language) formData.append('language', opts.language)\n  if (opts.prompt) formData.append('prompt', opts.prompt)\n\n  const res = await fetch(openAiApiUrl('/v1/audio/transcriptions'), {\n    method: 'POST',\n    headers: {\n      Authorization: `Bearer ${opts.apiKey}`,\n    },\n    body: formData,\n  })\n\n  if (!res.ok) {\n    const text = await res.text().catch(() => '')\n    throw new Error(`OpenAI HTTP ${res.status}: ${text.slice(0, 240)}`)\n  }\n\n  const json = (await res.json()) as any\n  return (json?.text as string | undefined) ?? ''\n}\n",
        "last_modified": "2026-01-13T12:35:11.845178"
      },
      "task_intent": {
        "title": "007-so-on-the-engine-right-here-where-i-chat-so-here-l",
        "description": "",
        "from_plan": false
      },
      "commits_behind_main": 3,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2026-01-13T12:35:11.349235",
  "last_updated": "2026-01-13T12:35:11.431784"
}